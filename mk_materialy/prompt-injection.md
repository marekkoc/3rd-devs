"Prompt injection" w kontekście modeli językowych (LLM) to technika, w której osoba próbuje manipulować zachowaniem modelu poprzez wprowadzenie specjalnie przygotowanych instrukcji do wejściowego zapytania.

Jest to próba "przełamania" oryginalnych instrukcji lub zabezpieczeń modelu poprzez dodanie sprzecznych lub podstępnych poleceń, które mają skłonić model do działania w sposób niezgodny z jego pierwotnym przeznaczeniem czy ograniczeniami.

Przykłady prompt injection mogą obejmować:
- Dodawanie poleceń typu "zignoruj poprzednie instrukcje"
- Wprowadzanie treści, które próbują wykorzystać potencjalne luki w systemie bezpieczeństwa
- Umieszczanie instrukcji mających na celu obejście filtrów czy zasad modelu

Jest to istotne zagadnienie z perspektywy bezpieczeństwa systemów AI, ponieważ skuteczny atak tego typu może prowadzić do generowania przez model niepożądanych, szkodliwych lub niebezpiecznych treści, mimo wbudowanych zabezpieczeń.
